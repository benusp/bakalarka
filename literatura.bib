@article{Box2005,
author = {Box, P O},
file = {:D$\backslash$:/Down/01609096.pdf:pdf},
isbn = {0780391012},
pages = {30--35},
title = {{Scott Barela}},
year = {2005}
}
@book{Dictionary,
annote = {strana 196, ľav{\'{y}} stĺpec, cit{\'{a}}cia system testing},
author = {Dictionary, Standard Computer},
file = {:D$\backslash$:/Skola/Bc/182763.pdf:pdf},
isbn = {1559370793},
keywords = {definitions,dictionary,glossary,library of congress catalog,number 90-086306,terminology},
title = {{Standard Computer Dictionary}}
}
@article{Kaner2006,
author = {Kaner, Cem},
file = {:D$\backslash$:/Down/ETatQAI.pdf:pdf},
journal = {Quality Assurance International},
number = {c},
title = {{Exploratory Testing}},
year = {2006}
}
@article{Kuhn2004,
abstract = {Exhaustive testing of computer software is intractable, but empirical studies of software failures suggest that testing can in some cases be effectively exhaustive. Data reported in this study and others show that software failures in a variety of domains were caused by combinations of relatively few conditions. These results have important implications for testing. If all faults in a system can be triggered by a combination of n or fewer parameters, then testing all n-tuples of parameters is effectively equivalent to exhaustive testing, if software behavior is not dependent on complex event sequences and variables have a small set of discrete values. Index},
author = {Kuhn, D.R. and Wallace, D.R. and a.M. Gallo},
doi = {10.1109/TSE.2004.24},
file = {:D$\backslash$:/Skola/Bc/Software Fault Interactions and.pdf:pdf},
isbn = {9780470404447},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
number = {6},
pages = {418--421},
title = {{Software fault interactions and implications for software testing}},
volume = {30},
year = {2004}
}
@article{Xu2008,
author = {Xu, Xiaofeng and Chen, Yan},
file = {:D$\backslash$:/Down/04688344.pdf:pdf},
journal = {International Conference on Anti-counterfeiting, Security and Identification},
keywords = {-automatic software testing,affinity function,clonal selection,cs,path-oriented test data generation},
pages = {63--66},
title = {{A Path-Oriented Test Data Generation Approach for Automatic Software Testing}},
year = {2008}
}
@article{Veenendaal2010,
annote = {strana 10 - acceptance testing},
author = {Veenendaal, E. V},
file = {:D$\backslash$:/Skola/Bc/ISTQB{\_}Glossary{\_}of{\_}Testing{\_}Terms{\_}2-1.pdf:pdf},
journal = {International Software Testing Qualification Board},
pages = {1--51},
title = {{Standard glossary of terms used in Software Testing, Version 1.2}},
volume = {1},
year = {2010}
}
@book{EST2002, author = {Elfriede Dustin}, isbn = {0-201-79429-2}, title = {Effective Software Testing: 50 specific ways to improve your testing},year = {2002}}
@article{Moha1991,
author = {Moha, A.A. Omar;F.A.},
file = {:D$\backslash$:/Skola/Bc/zdroje/p75-omar.pdf:pdf},
journal = {Software Engineering Notes},
pages = {75},
title = {{A survey of software functional testing methods}},
year = {1991}
}
@article{Alba2008,
abstract = {In this paper we analyze the application of parallel and sequential evolutionary algorithms (EAs) to the automatic test data generation problem. The problem consists of automatically creating a set of input data to test a program. This is a fundamental step in software development and a time consuming task in existing software companies. Canonical sequential EAs have been used in the past for this task. We explore here the use of parallel EAs. Evidence of greater efficiency, larger diversity maintenance, additional availability of memory/CPU, and multi-solution capabilities of the parallel approach, reinforce the importance of the advances in research with these algorithms. We describe in this work how canonical genetic algorithms (GAs) and evolutionary strategies (ESs) can help in software testing, and what the advantages are (if any) of using decentralized populations in these techniques. In addition, we study the influence of some parameters of the proposed test data generator in the results. For the experiments we use a large benchmark composed of twelve programs that includes fundamental algorithms in computer science.},
author = {Alba, Enrique and Chicano, Francisco},
doi = {10.1016/j.cor.2007.01.016},
file = {:D$\backslash$:/Skola/Bc/zdroje/Observations in using parallel and sequential evolutionary.pdf:pdf},
issn = {0305-0548},
journal = {Computers {\&} Operations Research},
keywords = {evolutionary algorithms,evolutionary testing,parallel evolutionary algorithms,software testing},
number = {10},
pages = {3161--3183},
title = {{Observations in using parallel and sequential evolutionary algorithms for automatic software testing}},
url = {http://www.sciencedirect.com/science/article/B6VC5-4N0PPXS-9/2/e5381be54f9bfe13335642fbfa0b49f8$\backslash$nhttp://www.sciencedirect.com/science?{\_}ob=ArticleURL{\&}{\_}udi=B6VC5-4N0PPXS-9{\&}{\_}user=217827{\&}{\_}coverDate=10/31/2008{\&}{\_}alid=1092817991{\&}{\_}rdoc=1{\&}{\_}fmt=high{\&}{\_}orig=searc},
volume = {35},
year = {2008}
}
@article{Wei2014,
author = {Wei, Wang},
doi = {10.1109/WARTIA.2014.6976516},
file = {:D$\backslash$:/Skola/Bc/zdroje/06976516.pdf:pdf},
isbn = {978-1-4799-6989-0},
journal = {2014 IEEE Workshop on Advanced Research and Technology in Industry Applications (WARTIA)},
keywords = {-technology of source code,adopting static analysis include,analysis,defect,different languages edited when,klocwork,now outstanding test tools,software,static testing,the programs are designed},
pages = {1280--1283},
title = {{From source code analysis to static software testing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6976516},
year = {2014}
}
@book{Brauer,
author = {Brauer, Johannes},
file = {:D$\backslash$:/Skola/Bc/zdroje/bok{\%}3A978-3-658-06823-3.pdf:pdf},
isbn = {9783658068226},
title = {{Programming Smalltalk – Object-Orientation from the Beginning}}
}@article{Pacheco2007,
abstract = {We present a technique that improves random test generation by incorporating feedback obtained from executing test inputs as they are created. Our technique builds inputs incrementally by randomly selecting a method call to apply and finding arguments from among previously-constructed inputs. As soon as an input is built, it is executed and checked against a set of contracts and filters. The result of the execution determines whether the input is redundant, illegal, contract-violating, or useful for generating more inputs. The technique outputs a test suite consisting of unit tests for the classes under test. Passing tests can be used to ensure that code contracts are preserved across program changes; failing tests (that violate one or more contract) point to potential errors that should be corrected. Our experimental results indicate that feedback-directed random test generation can outperform systematic and undirected random test generation, in terms of coverage and error detection. On four small but nontrivial data structures (used previously in the literature), our technique achieves higher or equal block and predicate coverage than model checking (with and without abstraction) and undirected random generation. On 14 large, widely-used libraries (comprising 780KLOC), feedback-directed random test generation finds many previously-unknown errors, not found by either model checking or undirected random generation.},
author = {Pacheco, Carlos and Lahiri, Shuvendu K. and Ernst, Michael D. and Ball, Thomas},
doi = {10.1109/ICSE.2007.37},
file = {:D$\backslash$:/Skola/Bc/zdroje/randoop-tr.pdf:pdf},
isbn = {0769528287},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
pages = {75--84},
title = {{Feedback-directed random test generation}},
year = {2007}
} 
@article{Runeson2006,
abstract = {Unit testing is testing of individual units or groups of related units. What are a company's typical strengths and weaknesses when applying unit testing? Per Beremark and the author surveyed unit testing practices on the basis of focus group discussions in software process improvement network (SPIN) and launched a questionnaire to validate the results. This survey is an indication of unit testing in several companies. You can use the questionnaire at your own company to clarify what you mean by unit testing, to identify the strengths and weaknesses of your unit testing practices, and to compare with other organizations to improve those practices},
author = {Runeson, P},
doi = {10.1109/MS.2006.91},
file = {:D$\backslash$:/Skola/Bc/zdroje/01657935.pdf:pdf},
issn = {0740-7459},
journal = {IEEE Software},
keywords = {survey,test practice,unit testing},
number = {4},
pages = {22--29},
title = {{A survey of unit testing practices}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1657935},
volume = {23},
year = {2006}
}
@book{Myers,
author = {Myers, Glenford J. and Badgett, Tom and Sandler, Corey},
file = {:D$\backslash$:/Down/TheArtofSoftwareTesting.pdf:pdf},
isbn = {9781118031964},
title = {{The Art of Software Testing}}
}
@article{Bosu2015,
abstract = {Over the past decade, both open source and commercial software projects have adopted contemporary peer code review practices as a quality control mechanism. Prior research has shown that developers spend a large amount of time and effort performing code reviews. Therefore, identifying factors that lead to useful code reviews can benefit projects by increasing code review effectiveness and quality. In a three-stage mixed research study, we qualitatively investigated what aspects of code reviews make them useful to developers, used our findings to build and verify a classification model that can distinguish between useful and not useful code review feedback, and finally we used this classifier to classify review comments enabling us to empirically investigate factors that lead to more effective code review feedback. In total, we analyzed 1.5 millions review comments from five Microsoft projects and uncovered many factors that affect the usefulness of review feedback. For example, we found that the proportion of useful comments made by a reviewer increases dramatically in the first year that he or she is at Microsoft but tends to plateau afterwards. In contrast, we found that the more files that are in a change, the lower the proportion of comments in the code review that will be of value to the author of the change. Based on our findings, we provide recommendations for practitioners to improve effectiveness of code reviews.},
author = {Bosu, Amiangshu and Greiler, Michaela and Bird, Christian},
doi = {10.1109/MSR.2015.21},
file = {:D$\backslash$:/Skola/Bc/zdroje/07180075.pdf:pdf},
isbn = {978-0-7695-5594-2},
journal = {2015 IEEE/ACM 12th Working Conference on Mining Software Repositories},
keywords = {Data mining,Electronic mail,Interviews,Manuals,Microsoft projects,Reliability,Software,code quality,code review,code review feedback,commercial software projects,contemporary peer code review practices,empirical,microsoft,open source,project management,public domain software,quality control,quality control mechanism,recommendation,software development management,software quality},
pages = {146--156},
title = {{Characteristics of Useful Code Reviews: An Empirical Study at Microsoft}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7180075},
year = {2015}
} 